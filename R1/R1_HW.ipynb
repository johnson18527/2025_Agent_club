{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgOU80G6LSnE"
      },
      "outputs": [],
      "source": [
        "!pip install  -q langchain langgraph transformers bitsandbytes langchain-huggingface langchain-community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryvvY71dvxh0"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "docs_text = \"\"\"\n",
        "火影代數\t姓名\t師傅\t徒弟\n",
        "初代\t千手柱間\t無明確記載\t猿飛日斬、水戶門炎、轉寢小春\n",
        "二代\t千手扉間\t千手柱間（兄長）\t猿飛日斬、志村團藏、宇智波鏡等\n",
        "三代\t猿飛日斬\t千手柱間、千手扉間\t自來也、大蛇丸、千手綱手（傳說三忍）\n",
        "四代\t波風湊\t自來也\t旗木卡卡西、宇智波帶土、野原琳\n",
        "五代\t千手綱手\t猿飛日斬\t春野櫻、志乃等（主要為春野櫻）\n",
        "六代\t旗木卡卡西\t波風湊\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\n",
        "七代\t漩渦鳴人\t自來也、旗木卡卡西\t木葉丸等（主要為木葉丸）\n",
        "\"\"\"\n",
        "\n",
        "docs = [Document(page_content=txt.strip()) for txt in docs_text.strip().split(\"\\n\\n\")]\n",
        "\n",
        "# chromadb 預設使用的大型語言模型為 \"all-MiniLM-L6-v2\"，由於該大型語言模型不支持中文，所以將模型替換為 \"infgrad/stella-base-zh-v3-1792d\"，並對 embedding 進行量化\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"infgrad/stella-base-zh-v3-1792d\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "persist_path = \"document_store\"\n",
        "collection_name = \"naruto_collection\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_path,\n",
        "    collection_name=collection_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OOS9LHkvxh0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# 使用 4-bit 量化模型\n",
        "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# 載入 tokenizer 與 4-bit 模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvNW71MfVQq8"
      },
      "source": [
        "### 1. 模型載入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFnvRVQpU7W4"
      },
      "source": [
        "使用的模型與 chatbot 教學同一顆"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD7juOvTU6to",
        "outputId": "a83e0123-1ef6-40e7-bf10-9fd91c7a44cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # 僅返回生成的回應內容\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myq54ykXoW0V"
      },
      "source": [
        "### 2. 定義狀態（State）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRCuRU3svPHl"
      },
      "source": [
        "這裡定義的 RAGState，包含了查詢文字（query）、檢索到的文件（docs）、以及最後生成的回答（answer）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pphh39DEoYNM"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict, List\n",
        "\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class RAGState(TypedDict):\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpylZY_mo7QF"
      },
      "source": [
        "### 3. 定義節點（Node）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knra8WF3M0r9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    # similarity_search 距離越小越相似\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": \"\"}\n",
        "\n",
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    query, docs = state[\"query\"], state[\"docs\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
        "        f\"內容：{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": output}\n",
        "\n",
        "def direct_generate_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    prompt = f\"請回答以下問題：{query}\\n\\n回答：\"\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": [], \"answer\": output}\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
        "# 定義 Route Node（決定走哪條路）\n",
        "def route_by_query(state):\n",
        "\n",
        "\n",
        "\n",
        "    query = state[\"query\"]\n",
        "    # 將字句轉換為向量\n",
        "    emb1 = model.encode(docs_text, convert_to_tensor=True)\n",
        "    emb2 = model.encode(query, convert_to_tensor=True)\n",
        "    # 計算相似度\n",
        "    path_from_cosine = util.pytorch_cos_sim(emb1, emb2).item()\n",
        "    print(f'cosine_similarity {path_from_cosine}')\n",
        "    choice = \"naruto\" if path_from_cosine > 0.72 else \"general\"\n",
        "    print(f\"跑到 → {choice}\")\n",
        "    return choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTk67tnLpM_N"
      },
      "source": [
        "### 4. 建立 LangGraph 流程圖（StateGraph）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G30TGyOVr-LV"
      },
      "source": [
        "在 LangGraph / LangChain 裡，RunnableLambda 就是用來把一個普通的 Python 函式（function）包裝成一個「Runnable」物件。\n",
        "\n",
        "Runnable 是 LangChain 裡的一個「標準介面」（Protocol Interface），代表「這個東西可以 .invoke()、可以被執行」。\n",
        "\n",
        "> ※ 換句話說：RunnableLambda 讓普通函式能接到 LangGraph 的 Node 上運行。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mLuCxndtTxr"
      },
      "source": [
        "為什麼第一個例子 chatbot 不需要使用 RunnableLambda ?\n",
        "\n",
        "- 如果只是單線到底（直直連接），不管有幾個節點，都不用自己包 RunnableLambda。\n",
        "\n",
        "- 只要涉及「判斷分流」、「根據條件走不同路線」，就一定要自己把判斷的 function 包成 RunnableLambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGMAM1ZypQzE"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 建立 LangGraph 流程圖\n",
        "graph_builder = StateGraph(RAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"condition\")\n",
        "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 設定條件分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"condition\",\n",
        "    path=RunnableLambda(route_by_query),\n",
        "    path_map={\n",
        "        \"naruto\": \"retriever\",\n",
        "        \"general\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 接下來的正常連接\n",
        "graph_builder.add_edge(\"retriever\", \"generator\")\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLWrWd1gVd6P"
      },
      "source": [
        "### 6. RAG 結果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeEqEVw2C88W",
        "outputId": "8219a37e-8fbf-4574-b534-f90716e7bb29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 誰是第四代火影?\n",
            "cosine_similarity 0.7422103881835938\n",
            "跑到 → naruto\n",
            "回答： 第四代火影是波風湊。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的師傅是誰?\n",
            "cosine_similarity 0.7497969269752502\n",
            "跑到 → naruto\n",
            "回答： 第四代火影的師傅是自來也。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的徒弟有哪些人?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cosine_similarity 0.752856433391571\n",
            "跑到 → naruto\n",
            "回答： 第四代火影的徒弟有旗木卡卡西、宇智波帶土、野原琳。\n",
            "============================================================ \n",
            "\n",
            "使用者: 相對論是誰發明的?\n",
            "cosine_similarity 0.5832839012145996\n",
            "跑到 → general\n",
            "回答： 相對論是愛因斯坦（Albert Einstein）在 1905 年提出的。\n",
            "============================================================ \n",
            "\n",
            "使用者: q\n",
            "掰啦！\n"
          ]
        }
      ],
      "source": [
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    # 設定初始 State\n",
        "    init_state: RAGState = {\n",
        "        \"query\": user_input,\n",
        "        \"docs\": [],\n",
        "        \"answer\": \"\"\n",
        "    }\n",
        "    # 呼叫 LangGraph\n",
        "    result = graph.invoke(init_state)\n",
        "    raw_output = result[\"answer\"]\n",
        "\n",
        "    answer_text = raw_output.split(\"回答：\")[-1].strip()\n",
        "    print(\"回答：\", answer_text)\n",
        "    print(\"===\" * 20, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxanAbfavxh3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzTOpgfkvxh3"
      },
      "source": [
        "# advance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMiMmlDBvxh3"
      },
      "source": [
        "改成能支援多輪問答（Multi-turn RAG），並能根據前面的query判斷問題。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i-4s3Oevxh3"
      },
      "source": [
        "> 請將 RAGState 加入 history 欄位，並在生成回答時，將歷史對話與當前問題一起組成 prompt。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPy5LQUGvxh3"
      },
      "source": [
        "> Hint：\n",
        "```\n",
        "class MultiTurnRAGState(TypedDict):  \n",
        "    history: List[str]  \n",
        "    query: str  \n",
        "    docs: List[Document]  \n",
        "    answer: str\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langchain langgraph transformers bitsandbytes langchain-huggingface langchain-community chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "docs_text = \"\"\"\n",
        "火影代數\t姓名\t師傅\t徒弟\n",
        "初代\t千手柱間\t無明確記載\t猿飛日斬、水戶門炎、轉寢小春\n",
        "二代\t千手扉間\t千手柱間（兄長）\t猿飛日斬、志村團藏、宇智波鏡等\n",
        "三代\t猿飛日斬\t千手柱間、千手扉間\t自來也、大蛇丸、千手綱手（傳說三忍）\n",
        "四代\t波風湊\t自來也\t旗木卡卡西、宇智波帶土、野原琳\n",
        "五代\t千手綱手\t猿飛日斬\t春野櫻、志乃等（主要為春野櫻）\n",
        "六代\t旗木卡卡西\t波風湊\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\n",
        "七代\t漩渦鳴人\t自來也、旗木卡卡西\t木葉丸等（主要為木葉丸）\n",
        "\"\"\"\n",
        "\n",
        "docs = [Document(page_content=txt.strip()) for txt in docs_text.strip().split(\"\\n\\n\")]\n",
        "\n",
        "# chromadb 預設使用的大型語言模型為 \"all-MiniLM-L6-v2\"，由於該大型語言模型不支持中文，所以將模型替換為 \"infgrad/stella-base-zh-v3-1792d\"，並對 embedding 進行量化\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"infgrad/stella-base-zh-v3-1792d\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "persist_path = \"document_store\"\n",
        "collection_name = \"naruto_collection\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_path,\n",
        "    collection_name=collection_name\n",
        "    #,collection_metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# 使用 4-bit 量化模型\n",
        "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# 載入 tokenizer 與 4-bit 模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # 僅返回生成的回應內容\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-LPY_kmAPfnc"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict, List\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class MultiTurnRAGState(TypedDict):\n",
        "    history: List[str]\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def retrieve_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query = state[\"query\"]\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    print({\n",
        "        \"query\": query,\n",
        "        \"docs\": docs,\n",
        "        \"answer\": \"\",\n",
        "        \"history\": state[\"history\"]\n",
        "    })\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"docs\": docs,\n",
        "        \"answer\": \"\",\n",
        "        \"history\": state[\"history\"]\n",
        "    }\n",
        "\n",
        "def generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query, docs, history = state[\"query\"], state[\"docs\"], state[\"history\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    history_prompt = \"\\n\".join([f\"Q: {history[i]}\\nA: {history[i+1]}\" for i in range(0, len(history), 2)])\n",
        "    full_prompt = (\n",
        "        f\"你是一個知識型助手，請根據以下歷史對話和內容回答問題：\\n\\n\"\n",
        "        f\"歷史對話：\\n{history_prompt}\\n\\n\"\n",
        "        f\"內容：\\n{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(full_prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"docs\": docs,\n",
        "        \"answer\": output,\n",
        "        \"history\": history + [query, output]\n",
        "    }\n",
        "\n",
        "def direct_generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query, history = state[\"query\"], state[\"history\"]\n",
        "\n",
        "    history_prompt = \"\\n\".join([f\"Q: {history[i]}\\nA: {history[i+1]}\" for i in range(0, len(history), 2)])\n",
        "    prompt = (\n",
        "        f\"你是一個聊天助手，請根據以下歷史對話回答問題：\\n\\n\"\n",
        "        f\"{history_prompt}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"docs\": [],\n",
        "        \"answer\": output,\n",
        "        \"history\": history + [query, output]\n",
        "    }\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "ebd_consin_model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
        "# 定義 Route Node（決定走哪條路）\n",
        "def route_by_query(state):\n",
        "\n",
        "\n",
        "\n",
        "    query = state[\"query\"]\n",
        "    # 將字句轉換為向量\n",
        "    emb1 = ebd_consin_model.encode(docs_text, convert_to_tensor=True)\n",
        "    emb2 = ebd_consin_model.encode(query, convert_to_tensor=True)\n",
        "    # 計算相似度\n",
        "    path_from_cosine = util.pytorch_cos_sim(emb1, emb2).item()\n",
        "    print(f'cosine_similarity {path_from_cosine}')\n",
        "    choice = \"naruto\" if path_from_cosine > 0.5 else \"general\"\n",
        "    print(f\"跑到 → {choice}\")\n",
        "    return choice\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 建立 LangGraph 流程圖\n",
        "graph_builder = StateGraph(MultiTurnRAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"condition\")\n",
        "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 設定條件分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"condition\",\n",
        "    path=RunnableLambda(route_by_query),\n",
        "    path_map={\n",
        "        \"naruto\": \"retriever\",\n",
        "        \"general\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 接下來的正常連接\n",
        "graph_builder.add_edge(\"retriever\", \"generator\")\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRZxOYLHvxh3",
        "outputId": "20876646-ecd3-44ed-d127-d8db55f8a41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 第四代火影是誰?\n",
            "cosine_similarity 0.7470983266830444\n",
            "跑到 → naruto\n",
            "{'query': '第四代火影是誰?', 'docs': [Document(metadata={}, page_content='火影代數\\t姓名\\t師傅\\t徒弟\\n初代\\t千手柱間\\t無明確記載\\t猿飛日斬、水戶門炎、轉寢小春\\n二代\\t千手扉間\\t千手柱間（兄長）\\t猿飛日斬、志村團藏、宇智波鏡等\\n三代\\t猿飛日斬\\t千手柱間、千手扉間\\t自來也、大蛇丸、千手綱手（傳說三忍）\\n四代\\t波風湊\\t自來也\\t旗木卡卡西、宇智波帶土、野原琳\\n五代\\t千手綱手\\t猿飛日斬\\t春野櫻、志乃等（主要為春野櫻）\\n六代\\t旗木卡卡西\\t波風湊\\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\\n七代\\t漩渦鳴人\\t自來也、旗木卡卡西\\t木葉丸等（主要為木葉丸）'), Document(metadata={}, page_content='火影代數\\t姓名\\t師傅\\t徒弟\\n初代\\t千手柱間\\t無明確記載\\t猿飛日斬、水戶門炎、轉寢小春\\n二代\\t千手扉間\\t千手柱間（兄長）\\t猿飛日斬、志村團藏、宇智波鏡等\\n三代\\t猿飛日斬\\t千手柱間、千手扉間\\t自來也、大蛇丸、千手綱手（傳說三忍）\\n四代\\t波風湊\\t自來也\\t旗木卡卡西、宇智波帶土、野原琳\\n五代\\t千手綱手\\t猿飛日斬\\t春野櫻、志乃等（主要為春野櫻）\\n六代\\t旗木卡卡西\\t波風湊\\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\\n七代\\t漩渦鳴人\\t自來也、旗木卡卡西\\t木葉丸等（主要為木葉丸）')], 'answer': '', 'history': []}\n",
            "AI 助理: 第四代火影是波風湊。\n",
            "======================================================================================================================== \n",
            "\n",
            "使用者: 他的師父是誰?\n",
            "cosine_similarity 0.6340187191963196\n",
            "跑到 → naruto\n",
            "{'query': '他的師父是誰?', 'docs': [Document(metadata={}, page_content='火影代數\\t姓名\\t師傅\\t徒弟\\n初代\\t千手柱間\\t無明確記載\\t猿飛日斬、水戶門炎、轉寢小春\\n二代\\t千手扉間\\t千手柱間（兄長）\\t猿飛日斬、志村團藏、宇智波鏡等\\n三代\\t猿飛日斬\\t千手柱間、千手扉間\\t自來也、大蛇丸、千手綱手（傳說三忍）\\n四代\\t波風湊\\t自來也\\t旗木卡卡西、宇智波帶土、野原琳\\n五代\\t千手綱手\\t猿飛日斬\\t春野櫻、志乃等（主要為春野櫻）\\n六代\\t旗木卡卡西\\t波風湊\\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\\n七代\\t漩渦鳴人\\t自來也、旗木卡卡西\\t木葉丸等（主要為木葉丸）'), Document(metadata={}, page_content='火影代數\\t姓名\\t師傅\\t徒弟\\n初代\\t千手柱間\\t無明確記載\\t猿飛日斬、水戶門炎、轉寢小春\\n二代\\t千手扉間\\t千手柱間（兄長）\\t猿飛日斬、志村團藏、宇智波鏡等\\n三代\\t猿飛日斬\\t千手柱間、千手扉間\\t自來也、大蛇丸、千手綱手（傳說三忍）\\n四代\\t波風湊\\t自來也\\t旗木卡卡西、宇智波帶土、野原琳\\n五代\\t千手綱手\\t猿飛日斬\\t春野櫻、志乃等（主要為春野櫻）\\n六代\\t旗木卡卡西\\t波風湊\\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\\n七代\\t漩渦鳴人\\t自來也、旗木卡卡西\\t木葉丸等（主要為木葉丸）')], 'answer': '', 'history': ['第四代火影是誰?', '第四代火影是波風湊。']}\n",
            "AI 助理: 他的師父是自來也。\n",
            "======================================================================================================================== \n",
            "\n",
            "使用者: 他還教了誰?\n",
            "cosine_similarity 0.5653490424156189\n",
            "跑到 → naruto\n",
            "{'query': '他還教了誰?', 'docs': [Document(metadata={}, page_content='火影代數\\t姓名\\t師傅\\t徒弟\\n初代\\t千手柱間\\t無明確記載\\t猿飛日斬、水戶門炎、轉寢小春\\n二代\\t千手扉間\\t千手柱間（兄長）\\t猿飛日斬、志村團藏、宇智波鏡等\\n三代\\t猿飛日斬\\t千手柱間、千手扉間\\t自來也、大蛇丸、千手綱手（傳說三忍）\\n四代\\t波風湊\\t自來也\\t旗木卡卡西、宇智波帶土、野原琳\\n五代\\t千手綱手\\t猿飛日斬\\t春野櫻、志乃等（主要為春野櫻）\\n六代\\t旗木卡卡西\\t波風湊\\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\\n七代\\t漩渦鳴人\\t自來也、旗木卡卡西\\t木葉丸等（主要為木葉丸）'), Document(metadata={}, page_content='火影代數\\t姓名\\t師傅\\t徒弟\\n初代\\t千手柱間\\t無明確記載\\t猿飛日斬、水戶門炎、轉寢小春\\n二代\\t千手扉間\\t千手柱間（兄長）\\t猿飛日斬、志村團藏、宇智波鏡等\\n三代\\t猿飛日斬\\t千手柱間、千手扉間\\t自來也、大蛇丸、千手綱手（傳說三忍）\\n四代\\t波風湊\\t自來也\\t旗木卡卡西、宇智波帶土、野原琳\\n五代\\t千手綱手\\t猿飛日斬\\t春野櫻、志乃等（主要為春野櫻）\\n六代\\t旗木卡卡西\\t波風湊\\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\\n七代\\t漩渦鳴人\\t自來也、旗木卡卡西\\t木葉丸等（主要為木葉丸）')], 'answer': '', 'history': ['第四代火影是誰?', '第四代火影是波風湊。', '他的師父是誰?', '他的師父是自來也。']}\n",
            "AI 助理: 波風湊除了教了旗木卡卡西、宇智波帶土和野原琳，他也可能是春野櫻的師父。在火影世界中，師徒關係可能是隱晦的，需要通過一些線索才能確認。\n",
            "======================================================================================================================== \n",
            "\n",
            "使用者: 相對論是他發明的嗎?\n",
            "cosine_similarity 0.6055086851119995\n",
            "跑到 → naruto\n",
            "{'query': '相對論是他發明的嗎?', 'docs': [Document(metadata={}, page_content='火影代數\\t姓名\\t師傅\\t徒弟\\n初代\\t千手柱間\\t無明確記載\\t猿飛日斬、水戶門炎、轉寢小春\\n二代\\t千手扉間\\t千手柱間（兄長）\\t猿飛日斬、志村團藏、宇智波鏡等\\n三代\\t猿飛日斬\\t千手柱間、千手扉間\\t自來也、大蛇丸、千手綱手（傳說三忍）\\n四代\\t波風湊\\t自來也\\t旗木卡卡西、宇智波帶土、野原琳\\n五代\\t千手綱手\\t猿飛日斬\\t春野櫻、志乃等（主要為春野櫻）\\n六代\\t旗木卡卡西\\t波風湊\\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\\n七代\\t漩渦鳴人\\t自來也、旗木卡卡西\\t木葉丸等（主要為木葉丸）'), Document(metadata={}, page_content='火影代數\\t姓名\\t師傅\\t徒弟\\n初代\\t千手柱間\\t無明確記載\\t猿飛日斬、水戶門炎、轉寢小春\\n二代\\t千手扉間\\t千手柱間（兄長）\\t猿飛日斬、志村團藏、宇智波鏡等\\n三代\\t猿飛日斬\\t千手柱間、千手扉間\\t自來也、大蛇丸、千手綱手（傳說三忍）\\n四代\\t波風湊\\t自來也\\t旗木卡卡西、宇智波帶土、野原琳\\n五代\\t千手綱手\\t猿飛日斬\\t春野櫻、志乃等（主要為春野櫻）\\n六代\\t旗木卡卡西\\t波風湊\\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\\n七代\\t漩渦鳴人\\t自來也、旗木卡卡西\\t木葉丸等（主要為木葉丸）')], 'answer': '', 'history': ['第四代火影是誰?', '第四代火影是波風湊。', '他的師父是誰?', '他的師父是自來也。', '他還教了誰?', '\\n波風湊除了教了旗木卡卡西、宇智波帶土和野原琳，他也可能是春野櫻的師父。在火影世界中，師徒關係可能是隱晦的，需要通過一些線索才能確認。']}\n",
            "AI 助理: 相對論是愛因斯坦在1905年所發明的。\n",
            "======================================================================================================================== \n",
            "\n"
          ]
        }
      ],
      "source": [
        "global_history: List[str] = []\n",
        "\n",
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    state = {\"history\": global_history, \"query\": user_input, \"docs\": [], \"answer\": \"\"}\n",
        "    result = graph.invoke(state)\n",
        "\n",
        "    answer = result[\"answer\"].split(\"回答：\")[-1].strip()\n",
        "    print(\"AI 助理:\", answer)\n",
        "    print(\"=\" * 120, \"\\n\")\n",
        "\n",
        "    global_history = result[\"history\"]\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b40e821630540f992f09f88125fdd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be4e23a62ec64a8d9dcd0df6573a9f98",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ca74c298bb149529b946cd33a3543f8",
            "value": 4
          }
        },
        "2b9ffdaf12104c91bf368963c6c3c9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ccd663fa8314f738dfb29466c0259d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38eada698ceb4ac785f572d387e9a6bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3aca735c5918469e83ad0663d94e2e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ca74c298bb149529b946cd33a3543f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cca752be91b4b8c90964740de3ccd49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fdf34e9c32d4bfd87102050d420b691": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ccd663fa8314f738dfb29466c0259d0",
            "placeholder": "​",
            "style": "IPY_MODEL_38eada698ceb4ac785f572d387e9a6bb",
            "value": " 4/4 [00:17&lt;00:00,  3.62s/it]"
          }
        },
        "911eea94fd07494fae82a46c7793e654": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f20646a370434b8c88d3fb2dc48ac0b3",
              "IPY_MODEL_0b40e821630540f992f09f88125fdd5c",
              "IPY_MODEL_7fdf34e9c32d4bfd87102050d420b691"
            ],
            "layout": "IPY_MODEL_4cca752be91b4b8c90964740de3ccd49"
          }
        },
        "be4e23a62ec64a8d9dcd0df6573a9f98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f20646a370434b8c88d3fb2dc48ac0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aca735c5918469e83ad0663d94e2e5b",
            "placeholder": "​",
            "style": "IPY_MODEL_2b9ffdaf12104c91bf368963c6c3c9ab",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
